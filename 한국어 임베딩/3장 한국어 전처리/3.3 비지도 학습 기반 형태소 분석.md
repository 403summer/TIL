# 3.3 비지도 학습 기반 형태소 분석



* 지도 학습 기법은 언어 전문가들이 직접 형태소 경계나 품사 정보를 모델에 가르쳐줘서 학습된 모델이다.
* **비지도 학습 (unsupervised learnig)** 기법은 데이터의 패턴을 모델 스스로 학습하게 함으로써 형태소를 분석하는 방법이다. 데이터에 자주 등장하는 단어들을 형태소로 인식한다.



## 3.3.1 soynlp 형태소 분석기



* **soynlp** (https://github.com/lovit/soynlp) 는 형태소 분석, 품사 판별 등을 지원하는 파이썬 기반 한국어 자연어 처리 패키지다. 데이터 패턴을 스스로 학습하는 비지도 학습 접근법을 지향하기 때문에 하나의 문장 혹은 문서에서보다는 어느 정도 규모가 있으면서 동질적인 문서 집합 (homogeneous documents) 에서 잘 작동한다.
* soynlp 패키지에 포함된 형태소 분석기는 데이터 통계량을 확인해 만든 단어 점수표로 작동한다. 단어 점수는 크게 **응집 확률 (Cohesion Probability)** 과 **브랜칭 엔트로피 (Branching Entropy)** 를 활용한다.
  * 주어진 문자열이 유기적으로 연결돼 함께 자주 나타나고 (응집 확률이 높을 때) , 그 단어 앞뒤로 다양한 조사, 어미 혹은 다른 단어가 등장하는 경우(브랜칭 엔트로피가 높을 때)  해당 문다열을 형태소로 취급한다.
  * ex) `꿀잼` 이라는 단어가 자주 나타나고 (응집 확률이 높다), `꿈잼` 앞에 `영화`, `정말`, `너무` 등 문자열이 `꿈잼` 뒤에 `ㅋㅋ`, `ㅎㅎ`, `!!` 등 패턴이 다양하게 나타났다면 (브랜칭 엔트로피가 높음),  `꿈잼`을  형태소로 취급한다.



**코드 3-22 soynlp 단어 점수 학습 `python`**

```python
from soynlp.word import WordExtractor

corpus_fname = "/notebooks/embedding/data/processed/processed_ratings.txt"
model_fname = "/notebooks/embedding/data/processed/soyword.model"

sentences = [sent.strip() for sent in open(corpus_fname,'r').readlines()]
word_extractor = WordExtractor (min_frequency=100,
                                min_cohesion_forward=0.05,
                                min_right_branching_entropy=0.0)
word_extractor.train(sentences)
word_extractor.save(model_fname)
```

* soynlp의 형태소 분석기는 우리가 가지고 있는 말뭉치의 통계량을 바탕으로 하기 때문에 별도의 학습 과정을 거쳐야 한다. 말뭉치의 분포가 어떻게 돼 있는지 확인하고 단어별로 응집 확률과 브랜칭 엔트로피 점수표를 만드는 절차가 필요하다는 이야기 이다.
* **WordExtractor** 클래스의 입력 타입은 하나의 요소가 문서 (문자열)인 리스트다.



**그림 3-16 soynlp 형태소 분석기의 분석 예시**

![그림3-16](images/그림3-16.png)

* 예를 들어 `코드 3-22` 로 학습된 단어 점수 표가 {"애비":0.5, "종":0.4} 라고 하면 `애비는 종이었다` 는 문자열은 `그림 3-16` 처럼 분석 된다.



**코드 3-23 soynlp 형태소 분석 `python`**

```python
import math
from soynlp.word import WordExtractor
from soynlp.tokenizer import LTokenizer

model_fname = "/notebooks/embedding/data/processed/soyword.model"

word_extractor = WordExtractor (min_frequency=100,
                                min_cohesion_forward=0.05,
                                min_right_branching_entropy=0.0)
word_extractor.load(model_fname)
scores = word_extractor.word_scores()
scores = {key:(scores[key].cohesion_forward * math.exp(scores[key].right_branching_entropy)) for key in scores.keys()}
tokenizer = LTokenizer(scores=scores)
tokens = tokenizer.tokenize("애비는 종이었다")

```

* `LTokenizer` 클래스는 입력 문장의 왼쪽부터 문자 단위로 슬라이딩해 가면서 단어 점수가 높은 문자열을 우선으로 형태소로 취급해 분리한다. 단 띄어쓰기가 돼 있다면 해당 어절을 단어로 인식한다. 한국어는 명사에 조사가 붙거나, 용언(형용사/동사)에 어미가 붙어 활용되는 교착어이기 때문에 왼쪽부터 슬라이딩해 가면서 분석해도 높은 품질을 기대할 수 있다.
* `코드 3-23` 은 `코드 3-22`에서 학습한 단어 점수표를 활용해 `애비는 종이었다`라는 문장을 하나의 형태소 분석하는 코드다.



**코드 3-24 soynlp 형태소 분석 전체 실행 스크립트 `bash`**

```bash
git pull origin master
bash preprocess.sh soy-tokenize
```

* 모든 과정을 자동으로 수행하려면 `코드 3-24`를 실행하면 된다.
* 결과는 `/notebooks/embedding/data/tokenized` 에 저장된다.



## 3.3.2 구글 센텐스피스



* 센텐스피스(sentencepiece) 는 구글(Kudo&Richardson, 2018) 에서 공개한 비지도 학습 기반 형태소 분석 패키지다. 1994년 제안된 **바이트 페어 인코딩 (BFE, Byte pair Encoding)** 기법 등을 지원한다.
* BPE의 기본 원리는 말뭉치에서 가장 많이 등장한 문자열을 병합해 문자열을 압축하는 것이다.
  * aaabdaaabac → `aa` 빈도수가 높다 이를 `Z`로 치환하자. → ZabdZabac
  * ZabdZabac → `ab` 빈도수가 높다 이를 `Y`로 치환하자. → ZYdZYac
* 자연어 처리에서 BPE가 처음 쓰인 것은 기계 번역 분야다. Sennrich et al. (2015) 은 BPF 알고리즘을 적용해 토크나이즈를 수행했다. BPE를 활용해 토크나이즈하는 메커니즘의 핵심은 이렇다. 우선 원하는 어휘 집합 크기가 될 때까지 반복적으로 고빈도 문자열들을 병합해 어휘 집합에 추가한다. 이것이 BPE 학습이다.
* 학습이 끝난 이후의 예측 과정은 이렇다. 문장 내 각 어절(띄어쓰기로 문장을 나눈 것)에 어휘집합에 있는 서브워드 (subword) 가 포함돼 있을 경우 해당 서브워드를 어절에서 분리한다(최장 일치 기준), 이후 어절의 나머지에서 어휘 집합에 있는 서브워드를 다시 찾고, 또 분리한다. 어절 끝까지 찾았는데 어휘 집합에 없으면 미등록 단어(unknown word) 로 취급한다.
  * 예를 들어 BPE를 학습하 결과 `학교`, `밥`, `먹었` 가 고빈도 서브워드라고 가정하자. 아래 문장은 다음과 같이 분석된다.
  * 학교에서 밥을 먹었다 →  _학교, 에서, _밥, 을, _먹었, 다
  * `_`로 시작하는 토큰은 해당 토큰이 어절의 시작이을 나타내는 구분자다.



**코드 3-25 BPF 학습 `python`**

```python
import sentencepiece as spm

train ='''--input=/notebooks/embedding/data/processed/processed_wiki_ko.txt\
          --model_prefix=sentpiece\
          --vocab_size=32000\
          --model_type=bpe\
          --character_coverage=0.9995'''

spm.SentencePieceTrainer.Train(train)
```

* 3.1 절에서 전처리를 수행한 한국어 위키백과 데이터를 가지고 BPE 알고리즘으로 어휘 집합(vocabulary) 을 만드는 코드다.



**코드 3-26 BERT 어휘 집합 만들기 `bash`**

```bash
git pull origin master
bash preprocess.sh make-bert-vocab
```

* BPE로 학습한 어휘 집합을 BERT (Devlin et al., 2018) 모델에도 쓸 수 있다. BPE는 문자열 기반의 비지도 학습 기법이기 때문에 데이터만 확보할 수 있다면 어떤 언어에든 적용이 가능하다.
* 센텐스피스 패키지 학습 결과를 BERT에 사용할 수 있는 어휘 집합으로 쓰기 위해 일부 후처리가 필요하다. `-`를 BERT에 맞게 바꾸고 `[PAD]`, `[UNK]`, `[MASK]`, `[SEP]` 등 스폐셜 토큰을 추가한다.
* `코드 3-26` 을 실행하면 BPE 학습부터 후처리까지 일괄 처리해 BERT 모델의 어휘 집합을 만든다. 



**코드 3-27 BERT FullTokenizer `python`**

```python
from models.bert.tokenization import FullTokenizer

vocab_fname = "/notebooks/embedding/data/processed/bert.vocab"
tokenizer = FullTokenizer(vocab_file=vocab_fname, do_lower_case=False)

tokenizer.tokenize("집에좀 가자")
```

* BERT 모델 코드에는 BPE로 학습한 어휘 집합으로 토큰을 분리하는 클래스(FullTokenizer) 가 포함돼 있다.



**그림 3-17 BERT의 토큰 분석 결과**

![그림3-17](images/그림3-17.png)

* FullTokenizer 분석 결과 `##`로 시작하는 토큰은 해당 토큰이 어절의 시작이 아님을 나타낸다.



## 3.3.3 띄어쓰기 교정



* soynlp에서는 띄어쓰기 교정 모듈도 제공한다. 이 모듈은 띄어쓰기 해턴을 학습한 뒤 해당 패턴대로 교정을 수행한다.
  * ex) 학습 데이터에서 `하자고` 문자열 앞뒤로 공백이 다수 발견됐다면 예측 단계에서 `하자고`가 등장했을 경우, `하자고` 앞뒤를 띄어서 교정하는 방식이다.
  * 교정 모델 역시 데이터의 통계량을 확인해야 하기 때문에 교정을 수행하기 전 학습이 필요하다.



**코드 3-28 soynlp 띄어쓰기 모듈 학습 `python`**

```python
from soyspacing.countbase import CountSpace

corpus_fname = "/notebooks/embedding/data/processed/processed_ratings.txt"
model_fname = "/notebooks/embedding/data/processed/space-correct.model"

model = CountSpace()
model.train(corpus_fname)
model.save_model(model_fname, json_format=False)
```

* 3.1절에서 전처리한 네이버 영화 리뷰 말뭉치를 활용해 soynlp 띄어쓰기 모듈을 학습하는 코드다.



**코드 3-29 soynlp 띄어쓰기 교정 `python`**

```python
from soyspacing.countbase import CountSpace

model_fname = "/notebooks/embedding/data/processed/space-correct.model"
model = CountSpace()
model.load_model(model_fname, json_format=False)
model.correct("어릴때보고 지금다시봐도 재밌어요")

#soynlp 띄어쓰기 교정 결과
어릴때 보고 지금 다시봐도 재밌어요
```

* soynlp 형태소 분석이나 BPE 방식의 토크나이즈 기법은 띄어쓰기에 따라 분석 결과가 크게 달라진다. 따라서 이들 모델을 학습하기 전 띄어쓰기 교정을 먼저 적용하면 그 분석 품질이 개선될 수 있다.



## 3.3.4 형태소 분석 완료된 데이터 다운로드



**코드 3-30 형태소 분석이 완료된 데이터 다운로드 `bash`**

```bash
git pull origin master
bash preprocess.sh dump-tokenized
```

* 시간을 절약하고 싶으면 `코드 3-30`을 실행해서 데이터를 다운로드하면 된다.
* `/notebooks/embedding/data/tokenized` 에 저장된다.



**표3-4 형태소 분석 완료된 데이터 목록**

| 파일명                | 내용                                                         | 관련 장 |
| --------------------- | ------------------------------------------------------------ | ------- |
| corpus_mecab_jamo.txt | 한국어 위키백과, 네이버 영화 말뭉치, KorQuAD를 합치고 은전한닢으로 형태소 분석을 한 뒤 자소로 분해한 데이터셋 | 4.3.3   |
| korquad_mecab.txt     | KorQuAD를 은전한닢으로 형태소 분석한 데이터셋                | 3.2.1   |
| ratings_hannanum.txt  | 네이버 영화 말뭉치를 한나눔으로 형태소 분석한 데이터셋       | 3.2.1   |
| ratings_khaiii.txt    | 네이버 영화 말뭉치를 Khaiii로 형태소 분석한 데이터셋         | 3.2.3   |
| ratings_komoran.txt   | 네이버 영화 말뭉치를 코모란으로 형태소 분석한 데이터셋       | 3.2.1   |
| ratings_mecab.txt     | 네이버 영화 말뭉치를 은전한닢으로 형태소 분석한 데이터셋     | 3.2.1   |
| ratings_okt.txt       | 네이버 영화 말뭉치를 Okt으로 형태소 분석한 데이터셋          | 3.2.1   |
| ratings_soynlp.txt    | 네이버 영화 말뭉치를 soynlp으로 형태소 분석한 데이터셋       | 3.2.1   |
| wiki_ko_mecab.txt     | 한국어 위키백과를 은전한닢으로 형태소 분석한 데이터셋        | 3.2.1   |

