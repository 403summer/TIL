# 4.5 GloVe



* GloVe는 Word2Vec 과 잠재 의미 분석 두 기법의 단점을 극복하고자 만들어진 모델이다. 잠재 의미 분석은 말뭉치 전체의 통계량을 모두 활용할 수 있지만, 그 결과물로 단어 간 유사도를 측정하기는 어렵다. 아울러 Word2Vec 기법이 단어 벡터 사이의 유사도를 측정하는 데는 LSA보다 유리하지만 사용자가 지정한 윈도우 내의 로컬 문맥(local context) 만 학습하기 때문에 말뭉치 전체의 통계 정보는 반영되기 어렵다는 단점을 지닌다 (GloVe dlgn 발표된 Skip-gram 모델이 말뭉치 전체의 글로벌한 통계랭인 SPMI 행렬을 분해하는 것과 동치라는 것을 증명하기도 했다 4.4.3절 참고)



## 4.5.1 모델 기본 구조



* '임베딩된 단어 벡터 간 유사도 측정을 수월하게 하면서도 말뭉치 전체의 통계 정보를 좀 더 반영해보자' 가 GloVe가 지향하는 핵심 목표라 말할 수 있을 것 같다.



**수식 4-20 GloVe의 목적함수**

![수식4-20](C:\Users\여름\Desktop\마크다운 정리\한국어 임베딩\4장 단어 수준 임베딩\images\수식4-20.png)



**그림 4-15 그림으로 이해하는 GloVe**

![그림4-15](C:\Users\여름\Desktop\마크다운 정리\한국어 임베딩\4장 단어 수준 임베딩\images\그림4-15.png)



* 임베딩된 두 단어 벡터의 내적이 말뭉치 전체에서의 동시 등장 빈도의 로그 값이 되도록 **목적함수(objective function)** 를 정의했다.

* 단어 i, j 각각에 해당하는 벡터 U~i~ , V~j~ 사이의 내적 값과 '두 단어 동시 등장 빈도의 로그값(logA~ij~)' 사이의 차이가 최소화될수록 학습 손실이 작아진다. 바이어스(bias) 항 두 개와 f(A~ij~)는 임베딩 품질을 높이기 위해 고안된 장치이며, |V|는 어휘 집합 크기다.
* GloVe는 우선 학습 말뭉치를 대상으로 단어 - 문맥 행렬 A를 만드는 것에서부터 학습을 시작한다. 어휘 집합 크기가 1만 개 정도 되는 말뭉치라면 요소 개수가 1억 (10000×10000)이나 되는 큰 행렬을 만들어야 한다. 이후 목적함수를 최소화하는 임베딩 벡터를 찾기 위해 행렬 분해를 수행해야 한다. 처음에 행렬 U, V를 랜덤으로 초기화한 뒤 `수식 4-20`을 최소화하는 방향으로 U, V를 조금씩 업데이트해 나간다. 학습 손실이 더 줄지 않거나 정해진 스텝 수만큼 학습했을 경우 학습을 종료한다. 학습이 끝나면 U를 단어 임베딩으로 쓸 수 있다. 이밖에 U+V^T^ , U와 V^T^ 를 이어 붙여 임베딩으로 사용하는 것도 가능하다.



## 4.5.2 튜토리얼



**코드 4-24 데이터 다운로드 `bash`**

```bash
git pull origin master
bash preprocess.sh dump-tokenized
```



**코드 4-25 데이터 합치기 `bash`**

```bash
cd /notebooks/embedding/data
cat tokenized/wiki_ko_mecab.txt tokenized/ratings_mecab.txt tokenized/korquad_mecab.txt > tokenized/corpus_mecab.txt
```



**코드 4-26 GloVe 학습 `bash`**

```bash
cd /notebooks/embedding
mkdir -p data/word-embeddings/glove
models/glove/build/vocab_count -min-count 5 -verbose 2 < data/tokenized/corpus_mecab.txt >data/word-embeddings/glove/glove.vocab
models/glove/build/cooccur -memory 10.0 -vocab-file data/word-embeddings/glove/glove.vocab -verbose 2 -window-size 15 < data/tokenized/corpus_mecab.txt > data/word-embeddings/glove/glove.cooc
models/glove/build/shuffle -memory 10.0 -verbose 2 data/word-embeddings/glove/glove.cooc > data/word-embeddings/glove/glove.shuf
models/glove/build/glove -save-file data/wprd-embeddings/glove/glove
-threads 4 -input-file data/word-embeddings/glove/glove.shuf -x-max 10 -iter 15
-vector-size 100 -binary 2 -vocab-file data/word-embeddings/glove/glove.vocab
-verbose 2

```

* GloVe는 C++ 구현체로 학습한다. 중요 파라미터는 임베딩 구축 대상 단어의 최소 빈도수(min-count), 단어 - 문맥 행렬을 만들 때 고려 대상 문맥 길이(window-size), 임베딩 차원 수(vector-size) 등이다. 학습이 완료되면 save-file 경로에 텍스트와 바이너리 파일 두 개가 생성된다.



**코드 4-27 Glove 모델의 코사인 유사도 상위 단어 목록 체크 `python`**

```python
from models.word_eval import WordEmbeddingEvaluator
model = WordEmbeddingEvaluator("data/word-embeddings/glove/glove.txt", method = "glove", dim = 100, tokenizer_name = "mecab")
model.most_similar("희망", topn = 5)
```

