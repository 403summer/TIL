# 4.3 FastText



* FastText는 각 단어를 문자(character) 단위 n - gram 으로 표현한다. 이 밖의 내용은 Word2Vec과 같다.



## 4.3.1 모델 기본 구조



**그림 4-11 '시나브로'의 n=3인 문자 단위 n-gram**



![그림4-11](C:\Users\여름\Desktop\마크다운 정리\한국어 임베딩\4장 단어 수준 임베딩\images\그림4-11.png)

* `<`,`>`는 단어의 경계를 나타내 주기 위해 FastText 모델이 사용하는 특수 기호다.



**수식 4-12 FastText의 단어 벡터 표현**



![수식4-12](C:\Users\여름\Desktop\마크다운 정리\한국어 임베딩\4장 단어 수준 임베딩\images\수식4-12.png)

* G~t~ 는 타깃 단어 t에 속한 문자 단위 n - gram 집합을 가리킨다.



**수식 4-13 FastText 모델에서 t, c가 포지티브 샘플(= t 주변에 c가 존재)일 확률**

![수식4-13](C:\Users\여름\Desktop\마크다운 정리\한국어 임베딩\4장 단어 수준 임베딩\images\수식4-13.png)

* FastText도 `수식 4-13`에 정의된 조건부 확률을 최대화하는 과정에서 학습된다.
* FastText도 Word2Vec과 같이 네거티브 샘플링 기법을 쓴다. FastText는 한발 더 나아가 타깃 단어(t), 문맥 단어(c) 쌍을 학습할 때 타깃 단어(t) 에 속한 문자 단위 n - gram 벡터(z)들을 모두 업데이트 한다.
* `수식 4-13`을 최대화하려면 분모를 최소화 시켜야 한다. 그러기 위해서는 z들과 v~c~ 간 내적 값을 높여야 한다. 벡터의 내적은 코사인 유사도와 비례한다.
* 예를 들어, `시나브로`가 타깃단어(t), `쌓였다`가 문맥단어의 포지티브 샘플(c) 이라면 `<시나`,`시나브`, `나브로`, `브로>`, `<시나브로>` 등 문자 n-gram 벡터(z)들 각각을 `쌓였다`에 해당하는 단어 벡터(v~c~)와의 유사도를 높인다.



**수식 4-14 FastText 모델에서 t, c가 네거티브 샘플(c를 t와 무관하게 말뭉치 전체에서 랜덤 샘플)일 확률**

![수식4-14](C:\Users\여름\Desktop\마크다운 정리\한국어 임베딩\4장 단어 수준 임베딩\images\수식4-14.png)

* 예를 들어, `시나브로`가 타깃단어(t), `컴퓨터`가 문맥단어의 네거티브 샘플(c) 이라면 `<시나`,`시나브`, `나브로`, `브로>`, `<시나브로>` 등 문자 n-gram 벡터(z)들 각각을 `컴퓨터`에 해당하는 단어 벡터(v~c~)와의 유사도를 낮춘다.



**수식 4-15 FastText 모델의 로그우도 함수**

![수식4-15](C:\Users\여름\Desktop\마크다운 정리\한국어 임베딩\4장 단어 수준 임베딩\images\수식4-15.png)

* FastText 모델이 최대화해야 할 로그우도 함수다. 모델을 한 번 업데이트할 때 1개의 포지티브 샘플(t~p~, c~p~)과 k개의 네거비트 샘플(t~n(i)~, c~n(i)~) 을 학습한다는 의미다.



## 4.3.2 튜토리얼



* FastText 또한 Word2Vec 과 동일한 말뭉치 (한국어 위키백과, 네이버 영화 리뷰 말뭉치, KorQuAD) 를 활용할 계획이다.



**코드 4-6 데이터 다운로드 `bash`**

```bash
git pull origin master
bash preprocess.sh dump-tokenized
```



**코드 4-7 데이터 합치기 `bash`**

```bash
cat data/tokenized/wiki_ko_mecab.txt data/tokenized/ratings_mecab.txt data/tokenized/korquad_mecab.txt > data/tokenized/corpus_.mecab.txt
```



**코드 4-8 FastText Skip-gram 모델 학습 `bash`**

```bash
mkdir -p data/word-embeddings/fasttext
models/fastText/fasttext skipgram -input data/tokenized/corpus_mecab.txt -output data/word-embeddings/fasttext/fasttext
```

* FastText는 페이스북에서 직접 구현한 C++ 코드를 실행해 임베딩을 학습한다.

* `/notebooks/embedding` 위치에서 실행하면 된다. 



**코드 4-9 FastText Skip-gram 모델의 코사인 유사도 상위 단어 목록 체크 `python`**

```python
from models.word_eval import WordEmbeddingEvaluator
model = WordEmbeddingEvaluator(
	vecs_txt_fname = "data/word-embeddings/fasttext/fasttext.vec",
	vecs_bin_fname = "data/word-embeddings/fasttext/fasttext.bin",
	method = "fasttext", dim = 100, tokenizer_name = "mecab")
model.most_similar("희망", topn = 5)    
```

* FastText 의 기본 임베딩 차원 수는 100이다.



**그림 4-12 '하였다'와 가장 유사한 FastText 단어 목록 `python`**

![그림4-12(2)](images/그림4-12(2).png)

* FastText 모델의 강점은 조사나 어미가 발달한 한국어에 좋은 성능을 낼 수 있다. FastText 모델로 학습하면 `그림 4-12`와 같이 용언(동사,형용사)의 활용이나 그와 관계된 어미들이 벡터 공간상 가깝게 임베딩 된다.
* 예컨대 `하였다`가 타깃 단어(t), `진행`이 문맥 단어의 포지티브 샘플(c)이라면 `<하였`, `하였다`, `였다>` 백터(z)들 각각이 `진행`에 해당하는 벡터(v~c~)와의 유사도가 높아진다. 이러한 방식으로 학습이 됐다면 `하였다` 벡터와 `하(다)`, `했(다)`, `(하)였으며` 등에 해당하는 벡터 간 유사도가 높을 것이다.



**그림 4-13 미등록 단어에 대한 FastText 임베딩 체크 `python`**

![그림4-13(2)](images/그림4-13(2).png)

* FastText는 오타나 **미등록 단어(unknown word)**에도 강건하다(robust). 각 단어의 임베딩을 문자 단위 n - gram 벡터의 합으로 표현하기 때문이다. 
* 예컨대 `그림 4-13`에서처럼 미등록 단어 `서울특별시`에 대해서도 FastText 임베딩을 추정할 수 있다. `서울특별시`는 `서울` 같은 문자 단위 n - gram 을 포함하고 있다. `서울`이 어휘 집합에 있다면 나머지 n - gram (울특, 특별 등) 이 모두 미등록 단어라 할지라도 `서울특별시`에 대한 임베딩을 추정할 수 있다. 다른 단어 임베딩 기법이 미등록 단어 벡터를 아예 추출할 수 없다는 사실을 감안하면 FastText는 경쟁력이 있다.



## 4.3.3 한글 자소와 FastText



* FastText는 문자 단위 n - gram 을 쓰기 때문에 한글과 궁합이 잘 맞는 편이다. 한글은 자소 단위로 분해할 수 있고, 이 자소 각각을 하나의 문자로 보고 FastText을 시행할 수 있기 때문이다.
* 이 절에서는 한국어 위키백과, 네이버 영화 리뷰 말뭉치, KorQuAD 세 가지 말뭉치를 은전한닢으로 형태소 분석을 시행한 뒤 이를 자소 단위로 분해한 데이터로 FastText 임베딩을 시행하는 튜토리얼을 진행한다.



**코드 4-10 데이터 다운로드 `bash`**

```bash
git pull origin master
bash preprocess.sh dump-tokenized
```



**코드 4-11 데이터 합치기 `bash`**

```bash
cd /notebooks/embedding
cat data/tokenized/wiki_ko_mecab.txt data/tokenized/ratings_mecab.txt data/tokenized/korquad_mecab.txt > data/tokenized/corpus_.mecab.txt
```



**코드 4-12 한글 자소분해 예시 `python`**

```python
from preprocess import jamo_sentence, get_tokenizer
tokenizer = get_tokenizer("mecab")
tokens = " ".join(tokenizer.morphs("나는 학교에 간다"))
print(jamo_sentence(tokens))

#결과
'ㄴㅏ-ㄴㅡㄴ ㅎㅏㄱㄱㅛ-ㅇㅔ- ㄱㅏㄴㄷㅏ-'
```

* 한글 텍스트를 자소 단위로 변환하는 예시다. 우선 변환 대상 문장을 은전한닢으로 형태소 분석을 한 뒤 각 토큰들을 공백과 함께 묶어준다. 이를 soynlp에서 제공하는 jamo_sentence 함수에 넣으면 `코드 4-12`와 같은 결과가 반환된다. 한글 한 글자를 초성, 중성, 종성 셋으로 분리하며 해당 요소가 없으면 -를 리턴한다. 알파벳이나 숫자, 기호 등은 그대로 출력한다.



**코드 4-13 은전한닢으로 형태소 분석된 말뭉치를 자소 단위로 분해 `bash`**

```bash
python preprocess/unsupervised_nlputils.py --preprocess_mode jamo \
	--input_path /notebooks/embedding/data/tokenized/corpus_mecab.txt \
	--output_path /notebooks/embedding/data/tokenized/corpus_mecab_jamo.txt
```

* 말뭉치 전체를 한꺼번에 자소 단위로 분해하는 스크립트이다.



**코드 4-14 자소 단위 FastText Skip-gram 모델 학습**

```bash
cd /notebooks/embedding
mkdir -p data/word-embeddings/fasttext-jamo
models/fastText/fasttext skipgram 
	-input data/tokenized/corpus_mecab_jamo.txt 
	-output data/word-embeddings/fasttext-jamo/fasttext-jamo
```

* 자소 단위로 분해한 말뭉치를 썼다는 점 빼고는 4.3.2절에서 설명한 FastText 임베딩 코드와 완전히 같다. FastText 모델은 각 자고를 하나의 문자로 보고 문자 단위 n -gram 임베딩을 한다.



**코드 4-15 FastText Skip-gram 모델의 유사어 상위 목록 체크 `python`**

```python
from models.word_eval import WordEmbeddingEvaluator
model = WordEmbeddingEvaluator(
	vecs_txt_fname = "data/word-embeddings/fasttext-jamo/fasttext-jamo.vec",
	vecs_bin_fname = "data/word-embeddings/fasttext-jamo/fasttext-jamo.bin",
	method = "fasttext-jamo", dim = 100, tokenizer_name = "mecab")
model.most_similar("희망")  
```

* 4.3.2절 기존 FastText에 썼던 유사도 체크 코드와 본질적으로는 같다. 다만 쿼리 단어의 자소 변환과 상위 유사 단어의 음절 원복 과정이 추가됐다.
* 구체적으로 설명하면 이렇다. 임베딩을 만들 때 자소 단위로 분해한 말뭉치를 썼기 때문에 어휘 집합에 속한 단어들 또한 자소로 분리돼 있다. 이 때문에 most_similar 함수에 인자로 넣을 쿼리 단어 또한 자소 단위로 분해한 뒤 FastText 임베딩을 추출해야 한다. 그 다음 이 임베딩에 대한 전체 단어 목록의 코사인 유사도를 구해 유사도 상위 단어들만 추린다. 마지막으로 이 단어들의 자소를 합쳐 다시 원래 단어로 복원 시킨다.



**코드 4-16 미등록 단어에 대한 자소 단위 FastText 임베딩 체크 `python`**

```python
model._is_in_vocabulary("서울특별시")
model.get_word_vector("서울특별시")
model.most_similar("서울특별시", topn=5)
```

* FastText 모델은 미등록 단어(unknown word)에 대한 임베딩을 추정할 수 있다. `서울특별시`라는 미등록 단어를 테스트하는 코드이다.