# 4.6 Swivel



**그림 4-16 그림으로 이해하는 Swivel**

![그림4-16](C:\Users\여름\Desktop\마크다운 정리\한국어 임베딩\4장 단어 수준 임베딩\images\그림4-16.png)

* Swivel은 PMI 행렬을 U와 V로 분해하고, 학습이 종료되면 U를 단어 임베딩으로 쓸 수 있다. 이밖에 U+V^T^ , U와 V^T^ 를 이어 붙여 임베딩으로 사용하는 것도 가능하다.



## 4.6.1 모델 기본 구조



**수식 4-21 Swivel의 목적함수 1 (말뭉치에 동시 등장한 케이스가 한 건이라도 있는 경우)**

![수식4-21](C:\Users\여름\Desktop\마크다운 정리\한국어 임베딩\4장 단어 수준 임베딩\images\수식4-21.png)

* Swivel은 PMI 행렬을 분해한다는 점에서 단어-문맥 행렬을 분해하는 GloVe와 다르다. Swivel은 목적함수를 PMI의 단점을 극복할 수 있도록 설계했다는 점 또한 눈에 띈다. 
* `수식 4-21`은 i라는 타깃 단어와 j라는 문맥 단어가 사용자가 정한 윈도우 내에서 단 한 건이라도 동시에 등장한 적이 있는 경우에 적용되는 목적함수다.
* `수식 4-21`을 직관적으로 이해하면 이렇다. 타깃 단어 i에 대응하는 U~i~ 벡터와 문맥 단어 j에 해당하는 V~j~ 벡터의 내적이 두 단어의 PMI 값과 일치하도록 두 벡터를 조금씩 업데이트한다. 여기서 f(x~ij~) 는 단어 i,j 의 동시 등장 빈도를 의미한다. f(x~ij~)가 클수록 U~i~ , V~j~ 벡터 간 내적 값이 실제 PMI 값과 좀 더 비슷해야 학습 손실이 줄어든다. 다시말해 단어 i, j가 같이 자주 등장할수록 두 단어에 해당하는 벡터의 내적이 PMI 값과 일치하도록 더욱 강제한다는 이야기다.



**수식 4-22 Swivel의 목적함수 2(말뭉치에 동시 등장한 케이스가 한 건도 없는 경우)**

![수식4-22](C:\Users\여름\Desktop\마크다운 정리\한국어 임베딩\4장 단어 수준 임베딩\images\수식4-22.png)

* `수식 4-22`는 단어 i, j가 말뭉치의 특정 윈도우 내에서 동시에 등장한 적이 한번도 없는 경우에 적용되는 목적함수다. 두 단어가 한 번도 동시에 등장하지 않았을 때 PMI는 음의 무한대로 발산하기 때문에 연구팀은 이 같은 케이스에 대해 목적함수를 별도로 설정했다. PMI^*^ 는 단어 i, j 의 동시 등장 횟수를 0 대신 1로 가정하고 계산한 PMI 값이다.



**수식 4-23 PMI**

![수식4-23](C:\Users\여름\Desktop\마크다운 정리\한국어 임베딩\4장 단어 수준 임베딩\images\수식4-23.png)

* `수식 4-22`를 좀 더 세밀하게 이해하기 위해 PMI 식을 다시 쓰면 `수식 4-23` 과 같다.
* A~ij~ 는 단어 i, j의 동시 등장 빈도, A~i*~ 는 i의 단독 빈도, A~*J~ 는 j의 단독 빈도, |D| 는 말뭉치의 길이(중복을 허용한 말뭉치 전체 토큰 수)



**수식 4-24 Swivel의 목적함수 2(말뭉치에 동시 등장한 케이스가 한 건도 없는 경우)**



![수식4-24](C:\Users\여름\Desktop\마크다운 정리\한국어 임베딩\4장 단어 수준 임베딩\images\수식4-24.png)

* `수식 4-24`는 `수식 4-22`에 `수식4-23`을 대입한 결과다.
* PMI^*^ 는 A~ij~ 를 1로 가정하고 계산한 것이기 때문에 logA~ij~ =log1=0이 돼 해당항이 소거된다. `수식 4-24`는 `수식4-22`와 비교해 그 표현이 달라졌을 뿐 계산 결과는 동치다.
* `수식 4-24`를 직관적으로 이해하면 이렇다. 연구팀은 단어 i, j가 각각 고빈도 단어인데 두 단어의 동시 등장 빈도가 0이라면 두단어는 정말고 같이 등장하지 않는, 의미상 무관계한 단어일 것이라고 가정했다. 예컨대 `무모`라는 단어와 `운전`이라는 단어는 단독으로는 자주 등장하는 단어이지만 어떤 말뭉치에서든 두 단어가 연어(collocation) 로 쓰이는 경우는 거의 없다. 이럴 땐 두 단어에 해당하는 벡터의 내적 값이 PMI^*^ 보다(=단 한 번 같이 등장했다고 가정했을 때 대비) 약간 작게 되도록 학습한다. `수식 4-24`를 보면 두 단어가 고빈도 단어라면 마지막 두 개 항(logA~i*~, logA~*j~)이 커지기 때문에 학습 손실을 줄이려면 U~i~, V~j~ 간 내적 값을 작게 해야 한다.
* 반대로 연구팀은 단어 i, j가 저빈도 단어인데 두 단어의 동시 등장 빈도가 0이라면 두 단어는 의미상 관계가 일부 있을 수 있다고 봤다. 우리가 가지고 있는 말뭉치 크기가 작아 어쩌다 우연히 해당 쌍의 동시 등장 빈도가 전혀 없는 걸로 나타났을 수도 있는 것이다. 예컨대  `확률`이라는 단어와 `분포`라는 단어는 아주 흔하지는 않지만 통계학과 관련 있는 데이터에서는 자주 같이 등장하는 편이다. 그런데 우리가 확보한 말뭉치가 네이버 영화 리뷰 데이터여서 이들의 동시 등장 빈도가 0임을 확인했다고 가정해보자. 이럴 땐 두 단어에 해당하는 벡터의 내적 값이 PMI^*^ 보다 (=단 한 번 같이 등장했다고 가정했을 때 대비) 약간 크게 되도록 학습한다. `수식 4-24`를 보면 두 단어가 저빈도 단어라면 마지막 두개 항(logA~i*~, logA~*j~)이 작아지기 때문에  U~i~, V~j~ 간 내적 값을 약간 크게 해도 학습 손실이 늘어나지 않는다.
* Swivel은 GloVe와 마찬가지로 U, V 행렬을 랜덤 초기화한 뒤 `수식 4-21`과 `수식 4-22`의 목적함수를 최소화하는 방향으로 행렬 값들을 조금씩 업데이트하는 방식으로 학습한다.



## 4.6.2 튜토리얼



**코드 4-28 데이터 다운로드 `bash`**

```bash
git pull origin master
bash preprocess.sh dump-tokenized
```



**코드 4-29 데이터 합치기 `bash`**

```bash
cd /notebooks/embedding/data
cat tokenized/wiki_ko_mecab.txt tokenized/ratings_mecab.txt tokenized/korquad_mecab.txt > tokenized/corpus_mecab.txt
```



**코드 4-30 Swivel 모델 학습 `bash`**

```bash
cd /notebooks/embedding
mkdir -p data/word-embeddings/swivel
models/swivel/fastprep --input data/tokenized/corpus_mecab.txt --output_dir data/word-embeddings/swivel/swivel.data
python models/swivel/swivel.py --input_base_path data/word-embeddings/swivel/swivel.data --output_base_path data/word-embeddings/swivel --dim 100
```

* `코드 4-30`은 연구팀이 직접 구현한 코드로, 입력행렬을 구축하는 부분은 C++, 행렬 분해 과정은 텐서플로로 작성돼 있다. 파이썬 코드(swivel.py)를 실행할 떄는 GPU 로 학습하는 것이 그 속도가 훨씬 빠르다.



**코드 4-31 Swivel 모델의 코사인 유사도 상위 목록 체크 `python`**

```python
from models.word_eval import WordEmbeddingEvaluator
model = WordEmbeddingEvaluator("data/word-embeddings/swivel/row_embedding", method = "swivel", dim = 100, tokenizer_name = "mecab")
model.most_similar("희망", topn = 5)
```

